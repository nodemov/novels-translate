#!/usr/bin/env python3
"""
Quick Token Check Script
A simple way to check token usage for your novel translation chunks
"""

import tiktoken
import sys
import os

def count_tokens(text: str) -> int:
    """Count tokens using tiktoken"""
    try:
        tokenizer = tiktoken.get_encoding("cl100k_base")
        return len(tokenizer.encode(text))
    except:
        # Fallback: approximate count
        return len(text) // 4

def analyze_chunk(text: str) -> dict:
    """Analyze a text chunk for translation"""
    # Your prompt template
    prompt_template = """à¸„à¸¸à¸“à¸„à¸·à¸­à¸™à¸±à¸à¹à¸›à¸¥à¸¡à¸·à¸­à¸­à¸²à¸Šà¸µà¸žà¸—à¸µà¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¹ƒà¸™à¸à¸²à¸£à¹à¸›à¸¥à¸™à¸´à¸¢à¸²à¸¢ Wuxia/Xianxia à¸ˆà¸µà¸™à¸ˆà¸²à¸à¸ à¸²à¸©à¸²à¸­à¸±à¸‡à¸à¸¤à¸©à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢

à¸à¸£à¸¸à¸“à¸²à¹à¸›à¸¥à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰à¸ˆà¸²à¸à¸ à¸²à¸©à¸²à¸­à¸±à¸‡à¸à¸¤à¸©à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢:

{text}

à¸«à¸¥à¸±à¸à¸à¸²à¸£à¹à¸›à¸¥:
1. à¸£à¸±à¸à¸©à¸²à¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢à¹à¸¥à¸°à¸šà¸£à¸£à¸¢à¸²à¸à¸²à¸¨à¸‚à¸­à¸‡à¸™à¸´à¸¢à¸²à¸¢ Wuxia/Xianxia à¹„à¸§à¹‰
2. à¹ƒà¸Šà¹‰à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸—à¸µà¹ˆà¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢à¹à¸¥à¸°à¹„à¸«à¸¥à¸¥à¸·à¹ˆà¸™
3. à¹à¸›à¸¥à¸¨à¸±à¸žà¸—à¹Œà¹€à¸‰à¸žà¸²à¸°: Cultivationâ†’à¸à¸²à¸£à¹€à¸žà¸²à¸°à¸žà¸´à¸–à¸µ, Qiâ†’à¸Šà¸µà¹ˆ, Dantianâ†’à¸•à¹‰à¸²à¸™à¹€à¸–à¸µà¸¢à¸™, Breakthroughâ†’à¸à¹‰à¸²à¸§à¸à¸£à¸°à¹‚à¸”à¸”, Elderâ†’à¸œà¸¹à¹‰à¸­à¸²à¸§à¸¸à¹‚à¸ª, Young Masterâ†’à¸„à¸¸à¸“à¸Šà¸²à¸¢à¸«à¸™à¸¸à¹ˆà¸¡
4. à¸„à¸‡à¸Šà¸·à¹ˆà¸­à¸•à¸±à¸§à¸¥à¸°à¸„à¸£à¹à¸¥à¸°à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆà¹€à¸‰à¸žà¸²à¸°à¹„à¸§à¹‰
5. à¸›à¸£à¸±à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸ à¸²à¸©à¸²à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸à¸±à¸šà¸œà¸¹à¹‰à¸­à¹ˆà¸²à¸™à¹„à¸—à¸¢
6. à¸£à¸±à¸à¸©à¸²à¸šà¸¸à¸„à¸¥à¸´à¸à¹à¸¥à¸°à¸ªà¹„à¸•à¸¥à¹Œà¸à¸²à¸£à¸žà¸¹à¸”à¸‚à¸­à¸‡à¸•à¸±à¸§à¸¥à¸°à¸„à¸£à¹„à¸§à¹‰

à¸à¸²à¸£à¹à¸›à¸¥:"""
    
    full_prompt = prompt_template.format(text=text)
    
    # Token counts
    system_tokens = count_tokens(prompt_template.replace("{text}", ""))
    input_tokens = count_tokens(text)
    total_input_tokens = count_tokens(full_prompt)
    estimated_output_tokens = int(input_tokens * 1.2)  # Thai output is usually longer
    total_tokens = total_input_tokens + estimated_output_tokens
    
    return {
        "text_length": len(text),
        "system_tokens": system_tokens,
        "input_tokens": input_tokens,
        "total_input_tokens": total_input_tokens,
        "estimated_output_tokens": estimated_output_tokens,
        "total_estimated_tokens": total_tokens
    }

def main():
    if len(sys.argv) > 1:
        # File provided as argument
        filename = sys.argv[1]
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                text = f.read()
            print(f"Analyzing file: {filename}")
        else:
            print(f"File not found: {filename}")
            return
    else:
        # Interactive mode
        print("Quick Token Checker")
        print("==================")
        text = input("Enter text to analyze (or filename): ").strip()
        
        if os.path.exists(text):
            with open(text, 'r', encoding='utf-8') as f:
                text = f.read()
            print(f"Analyzing file content...")
    
    analysis = analyze_chunk(text)
    
    print(f"\nðŸ“Š Token Analysis:")
    print(f"Text length: {analysis['text_length']:,} characters")
    print(f"System prompt: {analysis['system_tokens']} tokens")
    print(f"Input text: {analysis['input_tokens']} tokens")
    print(f"Total input: {analysis['total_input_tokens']} tokens")
    print(f"Estimated output: {analysis['estimated_output_tokens']} tokens")
    print(f"Total estimated: {analysis['total_estimated_tokens']} tokens")
    
    # Context limits
    current_limit = 16384  # Updated limit
    max_limit = 131072     # Model maximum
    
    print(f"\nðŸŽ¯ Context Analysis:")
    print(f"Current context limit: {current_limit:,} tokens")
    print(f"Model maximum: {max_limit:,} tokens")
    
    if analysis['total_estimated_tokens'] <= current_limit:
        print("âœ… Within current context limit")
    elif analysis['total_estimated_tokens'] <= max_limit:
        print("âš ï¸  Exceeds current limit but within model maximum")
        print(f"   Consider increasing num_ctx to {analysis['total_estimated_tokens'] + 1000}")
    else:
        print("âŒ Exceeds model maximum!")
        reduction_factor = max_limit / analysis['total_estimated_tokens']
        suggested_size = int(analysis['text_length'] * reduction_factor * 0.8)
        print(f"   Reduce chunk size to approximately {suggested_size} characters")
    
    # Chunk size recommendations
    if analysis['text_length'] > 2000:
        optimal_chunks = analysis['total_estimated_tokens'] // current_limit + 1
        chunk_size = analysis['text_length'] // optimal_chunks
        print(f"\nðŸ’¡ Recommendation:")
        print(f"Split into {optimal_chunks} chunks of ~{chunk_size} characters each")

if __name__ == "__main__":
    main()
